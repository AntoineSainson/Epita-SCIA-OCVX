{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthodes de déscentes de gradients\n",
    "\n",
    "Ce notebook est consacré à l'implémentation d'algorithmes de déscentes de gradients vus en cours. Nous abordons d'abord le problème sous un angle général en pointant les problèmes qu'on peut rencontrer dans ce cadre là. Dans la suite on s'attaque plutôt à des problématiques plus standards en analyse de données ; celles-ci ont des solutions dont le comportement est plus stable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo général\n",
    "\n",
    "L'algo général de déscente prend le format suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, ob_function, decay_function, tolerance, max_iter, d_direction, rate, hparams):\n",
    "    \"\"\"Gradient Descent.\n",
    "    \n",
    "    Computes minimal value of a convex function and local minimum of none convex function.\n",
    "    \n",
    "    Args:\n",
    "        x: initial starting point for descent.\n",
    "        ob_function: objective function of optimisation problem.\n",
    "        decay_function: function computing decay.\n",
    "        tolerance: float tolerance.\n",
    "        max_iter: upper bound on number of iterations.\n",
    "        d_direction: function computing descent direction\n",
    "        rate: function computing learning rate\n",
    "        hparams: hyperparameters for computing learning rate\n",
    "        \n",
    "    Output:\n",
    "        Couple minimizer, minimal value.\n",
    "        \n",
    "    \"\"\"\n",
    "    n_iter = 0\n",
    "    decay = tolerance + 5  #Make sure that we get into first loop\n",
    "    y = tmp_y = ob_function(x)\n",
    "    while decay > tolerance and n_iter < max_iter:\n",
    "        x = x - rate(ob_function, **hparams)*d_direction(ob_function, x)\n",
    "        y = ob_function(x)\n",
    "        decay = decay_function(tmp_y, y)\n",
    "        tmp_y = y\n",
    "        n_iter += 1\n",
    "    return (x, y) if decay <= tolerance else warn(\"Decay didn't get under tolerance rate.\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour être en mesure d'utiliser cet algorithme qui couvre l'essentiel des cas auxquels on peut faire face, il est nécessaire de définir les différents paramètres en entrées selon les déscentes qu'on souhaite implémenter.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le pas de déscente\n",
    "\n",
    "On a abordé trois manières de calculer le pas de déscente:\n",
    "\n",
    "  - Un pas constant.\n",
    "  - Calcul du pas optimal.\n",
    "  - Le backtracking.\n",
    "  \n",
    "Seul la dernière stratégie nécessite l'affinage éventuel d'hyperparamètres, les fameux hyperparamètres $\\alpha$ et $\\beta$. Par défaut on les fixent respectivement à $0.01$ et $0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directions de déscentes\n",
    "\n",
    "On a vu jusqu'à présent trois types de déscentes : \n",
    "\n",
    " - La déscente standard : la direction de déscente est celle du gradient.\n",
    " - La déscente de plus forte pente dans le cas de la norme l1 : la direction de déscente suit le vecteur de la base canonique de plus grande dérivée partielle.\n",
    " - L'algorithme de Newton où il s'agit de calculer l'inverse des hessiennes au point courant. \n",
    " \n",
    "Avant de s'intéresser à chacune de ces approches il nous faut être en mesure de calculer la valeur du gradient en un point courant. Tous les algos de déscente font usage de ce calcul. On commence donc par un apparté sur ce point qui reste relativement technique. \n",
    "\n",
    "**Dans la pratique quand on fait face à un problème spécifique, et pour lequel on peut calculer le gradient de la fonction objective explicitement, on effectue se calcule préalablement. Il est donc codé en dur.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculer la différentielle d'une fonction\n",
    "\n",
    "On a deux choix, le calcul numérique ou symbolique. Le second reconnaît la différentielle des fonctions usuelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer les directions de déscentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser la convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesurer la convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
